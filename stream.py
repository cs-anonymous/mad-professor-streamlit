import streamlit as st
from util.data_manager_clean import DataManager
# Add at the very top before other imports
import os
import json  # æ·»åŠ åœ¨æ–‡ä»¶é¡¶éƒ¨
from util.AI_professor_chat import AIProfessorChat
import uuid
import shutil

os.environ["TORCH_DISABLE_MLOCK"] = "1"  # Disable PyTorch memory locking

# åœ¨å¯¼å…¥ä¹‹åç«‹å³åˆå§‹åŒ–session state
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'is_chinese' not in st.session_state:
    st.session_state.is_chinese = True  # å°†æ­¤åˆå§‹åŒ–æå‰åˆ°æ–‡ä»¶é¡¶éƒ¨
# åœ¨ç°æœ‰session_stateåˆå§‹åŒ–åæ·»åŠ 
if 'ai_is_generating' not in st.session_state:
    st.session_state.ai_is_generating = False
if 'ai_current_request_id' not in st.session_state:
    st.session_state.ai_current_request_id = None
if 'ai_accumulated_response' not in st.session_state:
    st.session_state.ai_accumulated_response = ""
if 'selected_paper' not in st.session_state:
    st.session_state.selected_paper = None    
if 'ai_chat' not in st.session_state:
    st.session_state.ai_chat = AIProfessorChat()

# åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—
BASEDIR = os.path.dirname(os.path.abspath(__file__))
data_manager = DataManager(BASEDIR)
# æ›¿æ¢åŸæœ‰çš„ai_manageråˆå§‹åŒ–

# ä¿®æ”¹get_ai_responseä¸­çš„è°ƒç”¨
def get_ai_response(query, paper_id=None):
    try:
        print("âœ… [DEBUG] è¿›å…¥AIå“åº”æµç¨‹")  # æ§åˆ¶å°è¾“å‡º
        st.write("âœ… è¿›å…¥AIå“åº”æµç¨‹")
        print(f"ğŸ“¡ [DEBUG] è¯·æ±‚å‚æ•°: query='{query}', paper_id={paper_id}")  # å‚æ•°å¸¦å¼•å·é¿å…ç©ºæ ¼é—®é¢˜
        st.write(f"è¯·æ±‚å‚æ•°: query={query}, paper_id={paper_id}")
        
        if st.session_state.ai_is_generating:
            print("âš ï¸ [DEBUG] æ£€æµ‹åˆ°å·²æœ‰è¿›è¡Œä¸­çš„è¯·æ±‚ï¼Œè§¦å‘å–æ¶ˆ")
            st.warning("âš ï¸ æ£€æµ‹åˆ°å·²æœ‰ç”Ÿæˆä¸­çš„è¯·æ±‚ï¼Œæ­£åœ¨å–æ¶ˆ...")
            cancel_ai_response()
            
        # ç”Ÿæˆè¯·æ±‚ID
        st.session_state.ai_current_request_id = str(uuid.uuid4())
        st.session_state.ai_is_generating = True
        print(f"ğŸš€ [DEBUG] åˆ›å»ºæ–°è¯·æ±‚ID: {st.session_state.ai_current_request_id}")
        st.write(f"ğŸš€ åˆ›å»ºæ–°è¯·æ±‚ID: {st.session_state.ai_current_request_id}")
        
        # è·å–AIå“åº”
        print("ğŸ”„ [DEBUG] æ­£åœ¨è°ƒç”¨process_query_stream...")
        st.write("ğŸ”„ æ­£åœ¨è·å–AIå“åº”æµ...")
        response = st.session_state.ai_chat.process_query_stream(query, paper_id)
        
        # æ”¶é›†å®Œæ•´å“åº”
        full_response = ""
        for response_tuple in response:  # ä¿®æ”¹å˜é‡åä¸ºresponse_tuple
            sentence, emotion, scroll_info = response_tuple  # è§£åŒ…å…ƒç»„
            print(f"ğŸ“¥ [DEBUG] æ”¶åˆ°æµç‰‡æ®µ: {sentence}")  # åªè®°å½•æ–‡æœ¬å†…å®¹
            st.write(f"ğŸ“¥ æ”¶åˆ°å“åº”ç‰‡æ®µ: {sentence}")
            full_response += sentence
            yield sentence  # ä¿æŒåªè¿”å›æ–‡æœ¬å†…å®¹
            
        if not full_response:
            print("âŒ [ERROR] AIå“åº”ä¸ºç©ºï¼")
            st.error("âŒ AIå“åº”ä¸ºç©ºï¼Œè¯·æ£€æŸ¥AIæ¨¡å‹é…ç½®")
            
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        st.session_state.ai_accumulated_response = full_response
        print("ğŸ’¾ [DEBUG] å“åº”ä¿å­˜å®Œæˆ")
        st.write("ğŸ’¾ å“åº”ä¿å­˜å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ [CRITICAL] å¼‚å¸¸å‘ç”Ÿ: {str(e)}")
        st.error(f"âŒ AIå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()  # æ§åˆ¶å°è¾“å‡ºå®Œæ•´å †æ ˆ
        st.write(f"ğŸ” å®Œæ•´é”™è¯¯è¿½è¸ª:\n{traceback.format_exc()}")
    finally:
        st.session_state.ai_is_generating = False
        print("ğŸ›‘ [DEBUG] æ¸…ç†ç”ŸæˆçŠ¶æ€")
        st.write("ğŸ›‘ æ¸…ç†ç”ŸæˆçŠ¶æ€")

def cancel_ai_response():
    if st.session_state.ai_is_generating:
        # é‡ç½®çŠ¶æ€
        st.session_state.ai_is_generating = False
        st.session_state.ai_current_request_id = None

def change_seleted_paper():
    paper_id = st.session_state.selected_paper
    if paper_id:
        print(f"ğŸ“š åˆ‡æ¢åˆ°è®ºæ–‡: {paper_id}")
        paper_data = data_manager.load_rag_tree(paper_id)
        if paper_data:
            print("ğŸ“š åŠ è½½RAGæ ‘æˆåŠŸ")
            st.session_state.ai_chat.set_paper_context(paper_id, paper_data)

data_manager.load_papers_index()
# st.session_state.selected_paper = data_manager.papers_index[0]['id'] if data_manager.papers_index else None
# change_seleted_paper()

# åº”ç”¨åŸºç¡€é…ç½®
st.set_page_config(
    page_title="æš´èºçš„æ•™æˆè¯»è®ºæ–‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åœ¨ç°æœ‰å‡½æ•°åæ·»åŠ æ–‡ä»¶ä¸Šä¼ å›è°ƒå‡½æ•°
def handle_file_upload():
    if "uploaded_file" in st.session_state:
        uploaded_file = st.session_state.uploaded_file
        # ç¡®ä¿è·¯å¾„å¤„ç†æ­£ç¡®
        save_path = os.path.abspath(os.path.join("data", uploaded_file.name))
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        data_manager.upload_file(save_path)
        data_manager.is_paused = False
        data_manager.process_next_in_queue()
        # æ¸…é™¤ä¸Šä¼ çŠ¶æ€é˜²æ­¢é‡å¤è§¦å‘
        del st.session_state.uploaded_file

# ä¾§è¾¹æ  - è®ºæ–‡åˆ—è¡¨
with st.sidebar:
    with st.expander("ğŸ“š è®ºæ–‡åˆ—è¡¨", expanded=True):
        # è®ºæ–‡é€‰æ‹©
        selected_paper = st.selectbox(
            "é€‰æ‹©è®ºæ–‡",
            options=[p['id'] for p in data_manager.papers_index],
            format_func=lambda x: next(p['title'] for p in data_manager.papers_index if p['id'] == x),
            key='selected_paper',
            on_change=change_seleted_paper
        )
        col1, col2 = st.columns([1,1])
        with col1:
            if st.button("ğŸ“ ç¼–è¾‘è®ºæ–‡", key="edit_paper_btn"):
                st.session_state['selected_paper'] = selected_paper
        with col2:
            # åˆ é™¤è®ºæ–‡å¯¹åº”æ–‡ä»¶å¤¹
            if st.button("ğŸ—‘ï¸ åˆ é™¤è®ºæ–‡", key="delete_paper_btn"):
                paper_path = os.path.join(BASEDIR, 'output', st.session_state['selected_paper'])
                if os.path.exists(paper_path):
                    shutil.rmtree(paper_path)
                    st.rerun()
                else:
                    st.error("è®ºæ–‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

        
        # æ–‡ä»¶ä¸Šä¼ 
    
    
    # ä¿®æ”¹æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    with st.expander("ğŸš€ å¤„ç†é˜Ÿåˆ—", expanded=True):
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ è®ºæ–‡", 
            type=["pdf"],
            key="uploaded_file",
            on_change=handle_file_upload
        )
            
        
        # ä¿®æ”¹åçš„æ§åˆ¶æŒ‰é’®è¡Œ
        col1, col2 = st.columns([1,1])
        with col1:
            # åˆ‡æ¢å¼æŒ‰é’®
            if data_manager.is_paused:
                if st.button("â–¶ï¸ ç»§ç»­å¤„ç†", key="resume_btn"):
                    data_manager.is_paused = False
                    data_manager.process_next_in_queue()
                    st.rerun()
            else:
                if st.button("â¸ï¸ æš‚åœå¤„ç†", key="pause_btn"):
                    data_manager.pause_processing()
                    st.rerun()
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", key="refresh_db"):
                data_manager.load_papers_index()
                st.rerun()
        
        if data_manager.processing_queue:
            st.caption("å½“å‰å¤„ç†é˜Ÿåˆ—:")
            for item in data_manager.processing_queue:
                status_icon = "ğŸŸ¡" if item['status'] == 'incomplete' else "ğŸŸ¢"
                st.write(f"{status_icon} {item['id']} - {item['status']}")


    with st.expander("âš™ï¸ è®¾ç½®", expanded=True):
        setting_col1, setting_col2 = st.columns([1, 1])
        
        with setting_col1:
            # TTSå¼€å…³
            tts_enabled = st.checkbox("å¯ç”¨TTSè¯­éŸ³", value=True)
            
        with setting_col2:
            # ä¿æŒç°æœ‰ä»£ç ä¸å˜
            # æ³¨æ„ä¸€å®šè¦åœ¨main_colä¹‹å‰å®šä¹‰st.session_state['is_chinese']
            st.session_state['is_chinese'] = st.toggle("æ˜¾ç¤ºä¸­æ–‡", value=True)
            
    with st.expander("ğŸ’¬ AIå¯¹è¯", expanded=True):
        # èŠå¤©æ¶ˆæ¯æ˜¾ç¤º
        for msg in st.session_state.get("messages", []):
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
        
        # ç”¨æˆ·è¾“å…¥
        # ä¿®æ”¹åŸæœ‰çš„èŠå¤©è¾“å…¥å¤„ç†éƒ¨åˆ†
        if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            with st.spinner("æ•™æˆæ­£åœ¨æ€è€ƒ..."):
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                st.session_state.messages.append({"role": "user", "content": prompt})
                
                # æµå¼è·å–AIå“åº”
                response_container = st.empty()
                full_response = ""
                for chunk in get_ai_response(prompt, selected_paper):
                    full_response += chunk
                    response_container.markdown(full_response + "â–Œ")
                
                # æ·»åŠ æœ€ç»ˆå“åº”
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                st.rerun()

# ä¸»ç•Œé¢å¸ƒå±€
main_col = st.columns([10])[0]

# ä¿®æ”¹åŸæœ‰çš„æ¸²æŸ“éƒ¨åˆ†
with main_col:
    if selected_paper:
        paper = data_manager.load_paper_content(selected_paper)
        paper = {
            'metadata': paper[0],
            'zh_content': paper[1],
            'en_content': paper[2],
        }
        current_lang = 'zh' if st.session_state.is_chinese else 'en'
        content = paper[f"{current_lang}_content"]
        st.markdown(content, unsafe_allow_html=True)
                


with open("static/css/style.css", "r", encoding="utf-8") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# st.markdown(
#     """
#     <style>
#     /* .stApp { overflow: hidden; }   ç¦æ­¢æ»šåŠ¨æ¡ */
#     .katex { font-size: 1.2em !important; }  /* æ·»åŠ å…¬å¼å­—ä½“å¤§å°è°ƒæ•´ */
#     </style>
#     """,
#     unsafe_allow_html=True
